#include <iostream>
#include <format>

using namespace std;
int main(){
        
    /**
     * Задание 1. Работа с отладчиком. Базовые типы данных. Литералы.
     *
     * Выполняя программу по шагам, следите за значениями переменных и
     * интерпретируйте результат (помните, что количество байт, отводимых под
     * int, системо-зависимо).
     *
     * Обратите внимание на разную интерпретацию отладчиком signed и unsigned
     * целых типов данных, а также на внутреннее представление отрицательных
     * целых чисел.
     */
    
    char cByte = 'A'; //65 'A'
    cByte = 0x42; //66 'B'
    cByte = 66; //66 'B'
    cByte = -1; //-1 '\377'

    unsigned char ucByte = 0x41; //65 'A'
    ucByte = 'B'; //66 'B'
    ucByte = -1; //255 '\377' - 8ричное 255

    int iInt = 0xffffffff; //-1

    unsigned int uiInt = 0xffffffff; //4294967295

    float fFloat = 1.0;  //1
    double dDouble = 1.; //1

    /**
     * Выполните фрагмент, приведенный далее. В комментариях отразите, что
     * реально заносится в переменную. Объясните разницу между этим значением и
     * инициализатором.
     */

    double d = 0.1234567890123456789123456789; //0.12345678901234568 
    float  f = 0.1234567890123456789123456789; //0.123456791

    d = 1.; //1
    d = 0.999999999999999999999999999999999; //1

    /**
     * В комментариях напишите результат, возвращаемый оператором sizeof для
     * переменной типа wchar_t (ее размер)
     */

    wchar_t cw = L'Ф'; //1060 L'Ф'
    size_t n = sizeof(cw); //2 на винде, 4 через wls(шок)


    /**
     * Задание 2a. Неявное приведение типов данных. 
     *
     * Объясните разницу результата при выполнении (1) и (2): Покажите явно
     * (напишите в коде) преобразования, которые неявно выполняет компилятор
     */

    iInt = 1;
    double dDouble1 = (int) iInt / 3;     // (1) 0
    double dDouble2 = (double) iInt / 3.;    // (2) 0.33333333333333331

    /**
     * Ассоциативность операторов.
     *
     * Синтаксис языка C допускает "цепочечное" присваивание (как в строках (1)
     * и (2)). Посмотрев результаты выполнения строк (1) и (2) (значения
     * переменных dDouble, fFloat, nInt, sShort, cByte), определите порядок
     * выполнения присваиваний при цепочечной записи и объясните результат.
     *
     * Расставьте скобки, явно определяющие порядок выполнения, как это сделал
     * бы компилятор. Объясните (в комментариях) предупреждения (warnings)
     * компилятора.
     */

    short sShort;
    dDouble = fFloat = iInt = sShort = cByte = 3.3 / 3; // (1) справа налево, cByte1 = 1,тк целое, все остальные по цепочке
    
    cByte = sShort = iInt = fFloat = dDouble = 3.3 / 3; // (2) dDouble = 1.0999999999999999, fFloat = 1.10000002, далее приводятся типы, 1.10000002 округляется до 1

    /**
     * Ниже Вам дан пример "небрежного" использования неявного приведения
     * типов, что может привести к нежелательным результатам - объясните (в
     * комментариях), к каким?
     *
     * Напишите явно преобразования, которые неявно выполняет компилятор.
     */

    iInt = 256;
    cByte = (char) iInt; // останется 1, тк 257 = 10000001

    unsigned char cN1 = 255, cN2 = 2, cSum;
    cSum = cN1 + cN2; //

    /**
     * Сравните предыдущую строчку с приведенной ниже. 
     *
     * Объясните (в комментариях), почему в следующей строке не происходит
     * выход за разрядную сетку
     *
     * Напишите явно преобразования, которые неявно выполняет компилятор
     */

    int iSum = cN1 + cN2; //

    /**
     * Напишите, почему при сложении одинаковых значений (одинаковых в двоичной
     * системе) в строках (1) и (2) получаются разные результаты.
     *
     * Напишите явно преобразования, которые неявно выполняет компилятор и
     * объясните, что при этом происходит.
     */

    char c1 = 0xff, c2 = 2;
    unsigned char uc1 = 0xff, uc2 = 2;
    int iSum1 = c1 + c2;   //(1)
    int iSum2 = uc1 + uc2; //(2)

    
    /**
     * Задание 2b. Явное приведение типов данных.
     *
     * Проинтерпретируйте результат (значения переменной dDouble) в строке (3).
     *
     * Напишите явно преобразования, которые неявно выполняет компилятор.
     */

    int nTmp = 100, nn = 3;
    dDouble = 3.3 + nTmp / nn; // (3)

    /**
     * Получите результат без потери точности с помощью оператора явного
     * приведения типа.
     */

     // double dDouble3 = ...     // (4)



}